# F4 Modules enrich project files

## 1 What problem do modules solve?

A **module** is a self‑contained worker (Docker container) that analyses each file in the project and writes extra artefacts—for example EXIF metadata, captions, OCR text, thumbnails—into the Home‑Index metadata store:

```
output/metadata/by-id/<file‑hash>/<module‑name>/
└── version.json          # at minimum: {"version": <string|int>}
```

* Home‑Index schedules one job per **file × module**.
* Jobs run in parallel unless you configure resource share groups (see §2.3).
* If the module config changes, every file is re‑processed with the new version.

---

## 2 Configuring modules

### 2.1 Tell Home‑Index which modules exist

Set a single environment variable on the **home-index** service listing the queues to run:

```yaml
QUEUES: |
  - vision-module
  - text-module
```

*The YAML syntax is fixed; do **not** wrap it in quotes unless your shell demands it.*

### 2.2 Environment expected **inside** each module container

| Variable     | Required | Purpose                                                                                |
| ------------ | -------- | -------------------------------------------------------------------------------------- |
| `QUEUE_NAME` | ✔        | Base queue name – **must match an entry** in `QUEUES`.                        |
| `REDIS_HOST` | ✔        | Connection string (URL) of the shared Redis instance.                                  |
| `TIMEOUT`    | ✖        | Seconds after which Home‑Index retries a running job *(default 30)*.                   |
| `WORKER_ID`  | ✖        | Distinguishes multiple identical containers sharing one resource (e.g., the same GPU). |
| `RESOURCE_SHARES` | ✖ | YAML listing resource share groups so workers take turns. |

### 2.3 Sharing scarce resources

When containers compete for limited hardware or licences, set `RESOURCE_SHARES` inside each module container. The value is YAML describing one or more groups:

```yaml
- name: <share-group-1>
  seconds: <seconds>
- name: <share-group-2>
  seconds: <seconds>
```

Members of the same group run in round-robin order. Each worker processes jobs for up to `seconds` before yielding. A time-to-live prevents crashed workers from blocking the resource. Omit `RESOURCE_SHARES` when unrestricted parallelism is safe.


### 2.4 Reference template

A fully‑worked template lives in `features/F4/module_template/`.

---

## 3 Minimal `docker-compose.yml`

```yaml
services:
  home-index:
    image: ghcr.io/nashspence/home-index:latest
    environment:
      QUEUES: |
        - vision-module
        - text-module
      METADATA_DIRECTORY: /home-index/metadata
      REDIS_HOST: http://redis:6379
    volumes:
      - ./input:/files:ro          # project files
      - ./output:/home-index       # metadata + Meilisearch data
    depends_on: [meilisearch, vision-module, text-module, redis]

  meilisearch:
    image: getmeili/meilisearch:latest
    environment: [MEILI_NO_ANALYTICS=true]
    volumes:   [./output/meili:/meili_data]
    ports: ["7700:7700"]

  vision-module:
    build: {context: ./features/F4/module_template/, dockerfile: Dockerfile}
    environment:
      METADATA_DIRECTORY: /home-index/metadata
      QUEUE_NAME: vision-module
      REDIS_HOST: http://redis:6379
    volumes:
      - ./input:/files:ro
      - ./output:/home-index

  text-module:
    # … analogous …

  redis:
    image: redis:7
    command: redis-server --loglevel verbose
    ports: ["6379:6379"]
```

---

## 4 Acceptance criteria (platform‑agnostic)

For every scenario the stack may be run on Linux, macOS, or Windows (WSL).
`<module‑X>`, `<module‑Y>`, `<file‑A>` and `<file‑B>` are placeholders—any valid names can be used.

| #      | Scenario & pre‑conditions                                                                                                      | Steps (user actions → expected result)                                                                                                                                                                                                                                                        |
| ------ | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1**  | **Initial run – existing files are enriched**<br>Stack started with at least one module and at least one file already present. | 1 Start stack → `modules.log` created.<br>2 Wait → For every `<file‑A>` a `version.json` appears under the correct path.<br>3 Wait → Files become searchable in Meilisearch.                                                                                                                  |
| **2**  | **New file arrives**                                                                                                           | 1 Copy `<file‑B>` into `/files` while stack is running → Corresponding `version.json` appears soon after and file is in search index.                                                                                                                                                         |
| **3**  | **File contents change**                                                                                                       | 1 Replace any existing file’s bytes (hash changes).<br>2 Wait → A **new** metadata directory is created for the new hash; old directory remains intact.                                                                                                                                       |
| **4**  | **Multiple modules**                                                                                                           | 1 Stack started with `<module‑X>` and `<module‑Y>`.<br>2 Wait → Each file gains **two** `version.json` files (one per module).<br>3 If `<module‑X>` crashes, `<module‑Y>` still finishes its jobs.                                                                                            |
| **5**  | **Run‑phase timeout & retry**                                                                                                  | 1 Provide a module intentionally exceeding `TIMEOUT`.<br>2 Observe `modules.log` shows `START …` without `DONE`; no `version.json`.<br>3 Fix slowdown, restart container → Second `START` then `DONE`; `version.json` appears.                                                                |
| **6**  | **Check‑phase timeout & retry**                                                                                                | 1 Provide a module whose *check* stage can hang.<br>2 Queue length grows; `version.json` absent.<br>3 Fix issue, restart → Job reruns and completes; `version.json` present.                                                                                                                  |
| **7**  | **Warm restart – configuration unchanged**                                                                                     | 1 After any successful run, stop all containers.<br>2 Restart with **identical** `QUEUES` YAML → No new `START`/`DONE` lines; contents of `output/metadata` unchanged.                                                                                                                       |
| **8**  | **Warm restart – configuration changed**                                                                                       | 1 After Scenario 7, change the order or entries in `QUEUES`.<br>2 Restart → Every existing file is re‑processed; each metadata directory regenerated with new version.                                                                                                |
| **9**  | **Single share group rotates competing workers**
                                                          | 1 Run two containers of the **same module** with `RESOURCE_SHARES: "- name: gpu\n  seconds: 30"`.<br>2 Observe timestamps: jobs alternate—only one container runs at a time.
| **10** | **Multiple share groups enforced together**
                                                          | 1 Define one module with `RESOURCE_SHARES: "- name: gpu\n  seconds: 30\n- name: licence\n  seconds: 5"`.<br>2 Start two containers of that module.<br>3 Verify that a job in *either* container runs only when both groups allow—no interleaving occurs where one worker monopolises a resource. |

All scenarios **must pass without changing the spec text**—only the concrete module names, file names, and resource share group names vary per project.

---

**End of specification**
